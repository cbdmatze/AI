'''

**********************************************************************************************************
                        🧬_NLP Terms, Text Preprocessing, Representing Text, and NLP Tasks_🧬
**********************************************************************************************************


🧬  In the previous chapter, we explored what Generative AI and Natural Language Processing (NLP) are, 
    their significance, and key technologies shaping the field.



🧬  Now, as you progress in understanding NLP, it’s crucial to learn how to prepare and represent text 
    data in a way that machines can understand. This chapter introduces the foundational steps 
    of text preprocessing, different methods for representing text as data, and some basic NLP 
    tasks that you can begin experimenting with.


**********************************************************************************************************
                                                🧬_Document_🧬
**********************************************************************************************************


🧬  Objective:

    By the end of this section, you should have a better understanding of the basic terms used in NLP.



🧬  What is Document?

    A document is a single piece of text, which can be anything from a single sentence to an entire book. It is the basic unit of text that NLP models process.

    Documents can be diverse in nature, such as emails, web pages, articles, or tweets.



🧬  Example:
    · A single news article from a newspaper.
    · A tweet: “Just watched an amazing movie!”
    · An email: “Dear John, I hope this email finds you well…”


**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬  A corpus (plural: corpora) is a large collection of documents. It serves as the dataset on which 
    NLP models are trained and evaluated. A corpus typically contains documents that are related 
    by topic, language, or genre and is used to analyze linguistic patterns and build statistical models.



🧬  Example:
    A collection of all articles from a specific newspaper over a year.
    A dataset of customer reviews from an e-commerce website.
    The Gutenberg Corpus: A collection of literary texts from Project Gutenberg.




🧬  Feature


🧬  A feature is a measurable property or characteristic of the text that is used in machine learning  
    models.



🧬  Features are extracted from documents and can represent various aspects of the text, 
    such as the presence of specific words, the length of sentences, or the occurrence of particular 
    patterns.


**********************************************************************************************************
                                        🧬_Text Preprocessing_🧬
**********************************************************************************************************


🧬  Objective:

    By the end of this section, you should be able to explain what text preprocessing is, 
    why it’s important, and how to apply common preprocessing techniques to raw textual data.

    
**********************************************************************************************************
                                    🧬_Definition and Overview_🧬
**********************************************************************************************************


🧬  What is Text Preprocessing?

    In text processing or preprocessing, we apply a series of steps and transformations to raw text to make 
    it more suitable for analysis, modeling, and other NLP tasks.

    Whay is processing needed? Well, natural language is full of complexities - misspelling, diverse writing
    styles, varying puctuation - wich can hinder the performance of NLP models.

    That's why a given text first has to be streamlined into cleaner, standardized format, before it can be
    processsed by a language model.

    

🧬  Importance of Preprocessing in NLP Workflows:

    Improving Model Accuracy:
    Models trained on clean, consistent input data gernerally yield better accuracy and more robust predictions.


    Reducing Noise:
    Removing unecessary elements such as non-informative words (e.g., "the", "is", "and") or punctuation 
    simplifies patterns in the data, making patterns clearer.

    
    Efficiency:
    Preprocessing makes the data more compact and can reduce computational cost, enabling models to train and 
    predict faster.

    
    Generalization:
    Well-preprocessed data is easier for models to understand, enabling them to generalize across different
    datasets and tasks.



**********************************************************************************************************
                                            🧬_Core Techniques_🧬
**********************************************************************************************************


🧬  Tokenization: Splitting Sentences into Words


    Definition:
    Tokenization is the process of splitting text into smaller units called 'tokens'. Tokens can be whole
    words, but they also can be subwords or characters.


    Example:
    The sentence "Hello world!" can be tokenized into the following words: ["Hello", "world", "!"].


    Benefits:
    Allows models and algorithms to work on meaningful units (words or subwords) rather than entire sentences.




🧬  Stop word Removal, Lowercasing, and Punctuation Removal


    Stop word Removal: Stop words (e.g., "the", "and", "to") are common words that don't usually contribute
    much to the meaning of text. Removing them can reduce noise.


    Lowercasing:
    Converting all letters to lowercase ensures that words like "Apple" and "apple" are treated the same.


    Punctuation Removal:
    Removing punctuation can simplify text analysis, although in some tasks punctuation might carry meaning
    (like sentiment from exclamation marks!). The decision depends on the application.




🧬  Stemming and Lemmatization: Reducing Words to Their Root Forms


    Stemming:
    A heuristic method that chops off word endings to reduce a word to a base form (stem). For example, 
    "playing", "played", and "plays" would all be reduced to "play". Stemming can be crude and may not 
    always produce valid words.


    Lemmatization:
    A more intelligent approach that reduces words to their dictionary base form (lemma). It considers
    the part-of-speech and ensures the reduced form is an actual word. For example, "better" may lemmatize
    to "good" if you consider it as an adjective.


    
🧬  Introduction to Regular Expressions (Regex) for Text Manipulation


    Rexex Basics:
    Regular expressions are a mini-language used to find, match, and manipulate specific text patterns. 
    For instance, you can use a regex to identify all sequences of digits in a text and remove them.


    Common Uses:
    Cleaning out URLs, removing special characters, extracting certain word patterns, or validating 
    formats like email addresses.


🧬


🧬


🧬


🧬


**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬


🧬




**********************************************************************************************************
🧬__🧬
**********************************************************************************************************
'''